---
layout: page
title: Distributing the Llama LLM with Wrapyfi
description: Wrapyfi enables distributing LLaMA (inference only) on multiple GPUs/machines, each with less than 16GB VRAM
img: assets/img/covers/llama_wrapyfi_cover.png
redirect: https://github.com/modular-ml/wrapyfi-examples_llama
importance: 4
category: OSS
---
